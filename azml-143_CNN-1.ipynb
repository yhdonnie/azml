{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.fashion",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bYJbFFmpNYacptFudBHkGHdBmSTjSPGV",
      "authorship_tag": "ABX9TyMgAZKSquo8AXHhgw3T+Gk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhdonnie/azml/blob/circleci-project-setup/azml-143_CNN-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYKgQokREsL9"
      },
      "source": [
        "### Keras CNN 으로 패션 아이템 구분하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lEWU_axE7CM"
      },
      "source": [
        "1. 패키지 수입 및 파라미터 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-YM4bKgErrh"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from time import time\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.utils import np_utils\r\n",
        "\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dense, MaxPool2D\r\n",
        "from keras.layers import Conv2D, InputLayer\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRuk8YrZGh00"
      },
      "source": [
        "# 파라미터\r\n",
        "MY_EPOCH=100\r\n",
        "MY_BATCH=500"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSulTu5ZJu4s"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWDGwd7JJzkc",
        "outputId": "5c5ee8a8-4f5f-4f0f-ae5f-d2aa9afa3786"
      },
      "source": [
        "# 데이터 불러오기\r\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\r\n",
        "print('학습용 입력 데이터', X_train.shape)\r\n",
        "print('학습용 출력 데이터:', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터', X_test.shape)\r\n",
        "print('평가용 출력 데이터:', Y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 (60000, 28, 28)\n",
            "학습용 출력 데이터: (60000,)\n",
            "평가용 입력 데이터 (10000, 28, 28)\n",
            "평가용 출력 데이터: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GDroJx4mLfB0",
        "outputId": "2ce8a307-9258-4ff7-9b42-78ba809f3e1f"
      },
      "source": [
        "# 데이터 샘플 출력.. 범위 0-255 (8 bit)\r\n",
        "print('학습용 데이터 첫번째 이미지 화소 정보')\r\n",
        "print(X_train[0])\r\n",
        "plt.figure(figsize=(10,10))\r\n",
        "plt.imshow(X_train[0], cmap='gray')\r\n",
        "print('학습용 데이터 첫번째 이미지의 라벨',Y_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 데이터 첫번째 이미지 화소 정보\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "학습용 데이터 첫번째 이미지의 라벨 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvbFkcJxNP6G",
        "outputId": "4e054709-b702-4d69-9361-4031295aa6d2"
      },
      "source": [
        "# 데이터 스케일링 \r\n",
        "X_train = X_train / 255.0\r\n",
        "print(X_train[0])\r\n",
        "X_test = X_test /255.0\r\n",
        "\r\n",
        "# 이미지 채널 정보 추가 (흑백=1, 칼라=3  RGB)\r\n",
        "X_train = X_train.reshape(60000, 28, 28, 1)\r\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
            "  0.         0.00392157 0.01568627 0.         0.         0.\n",
            "  0.         0.00392157 0.00392157 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
            "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
            "  0.01568627 0.         0.         0.01176471]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
            "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
            "  0.         0.04705882 0.03921569 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
            "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
            "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
            "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
            "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
            "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
            "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
            "  0.48235294 0.76862745 0.89803922 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
            "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
            "  0.8745098  0.96078431 0.67843137 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
            "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
            "  0.8627451  0.95294118 0.79215686 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.01176471 0.\n",
            "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
            "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
            "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.02352941 0.\n",
            "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
            "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
            "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01568627 0.         0.\n",
            "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
            "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
            "  0.85098039 0.81960784 0.36078431 0.        ]\n",
            " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
            "  0.00784314 0.         0.         0.         0.         0.\n",
            "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
            "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
            "  0.85490196 1.         0.30196078 0.        ]\n",
            " [0.         0.01176471 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
            "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
            "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
            "  0.87843137 0.95686275 0.62352941 0.        ]\n",
            " [0.         0.         0.         0.         0.07058824 0.17254902\n",
            "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
            "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
            "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
            "  0.91372549 0.93333333 0.84313725 0.        ]\n",
            " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
            "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
            "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
            "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
            "  0.8627451  0.90980392 0.96470588 0.        ]\n",
            " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
            "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
            "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
            "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
            "  0.87058824 0.89411765 0.88235294 0.        ]\n",
            " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
            "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
            "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
            "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
            "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
            " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
            "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
            "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
            "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
            "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
            " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
            "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
            "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
            "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
            "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
            " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
            "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
            "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
            "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
            "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
            " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
            "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
            "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
            "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
            "  0.75294118 0.84705882 0.66666667 0.        ]\n",
            " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
            "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
            "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
            "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
            "  0.38823529 0.22745098 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
            "  0.1372549  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9blKv27XPQY0",
        "outputId": "520ad206-484c-4a23-c9b7-2d3adc29c5a8"
      },
      "source": [
        "#  라벨 정보 수정 (\"one hot encoding\") 이진수로 바꿈\r\n",
        "Y_train = np_utils.to_categorical(Y_train, 10)\r\n",
        "print('학습용 데이터 첫번째 이미지의 라벨',Y_train[0])\r\n",
        "Y_test = np_utils.to_categorical(Y_test,10)\r\n",
        "\r\n",
        "print('학습용 입력 데이터', X_train.shape)\r\n",
        "print('학습용 출력 데이터:', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터', X_test.shape)\r\n",
        "print('평가용 출력 데이터:', Y_test.shape)\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 데이터 첫번째 이미지의 라벨 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "학습용 입력 데이터 (60000, 28, 28, 1)\n",
            "학습용 출력 데이터: (60000, 10)\n",
            "평가용 입력 데이터 (10000, 28, 28, 1)\n",
            "평가용 출력 데이터: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXr71Cp2RtLV"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHWOi4qJRyr1",
        "outputId": "17eae779-8de6-4bd7-a025-d397ff51d434"
      },
      "source": [
        "# CNN 구현\r\n",
        "model = Sequential()\r\n",
        "model.add(InputLayer(input_shape=(28,28,1)))\r\n",
        "\r\n",
        "# 첫번째 합성곱 블럭 (conv+pooling)\r\n",
        "model.add(Conv2D(filters=32,\r\n",
        "                 kernel_size=2, \r\n",
        "                 padding='same',\r\n",
        "                 activation='relu'))\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "\r\n",
        "# 두번째 합성곱 블럭 (conv+pooling)\r\n",
        "model.add(Conv2D(filters=64,\r\n",
        "                 kernel_size=2, \r\n",
        "                 padding='same',\r\n",
        "                 activation='relu'))\r\n",
        "\r\n",
        "model.add(MaxPool2D(pool_size=2))\r\n",
        "\r\n",
        "# DNN 입성 (flatten 을 사용하여 은닉층으로 들어감)\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "model.add(Dense(units=128,\r\n",
        "                activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(units=10,\r\n",
        "                activation='softmax'))\r\n",
        "\r\n",
        "print('CNN요약')\r\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 411,242\n",
            "Trainable params: 411,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4kE_SqdbBY"
      },
      "source": [
        "4. 인공신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrITbyIrdiof",
        "outputId": "55cc72dd-6661-4d00-943f-9ced04e7f6cd"
      },
      "source": [
        "#  학습환경, 학습 방식 설정\r\n",
        "\r\n",
        "model.compile(optimizer='adam',\r\n",
        "#model.compile(optimizer='sgd',\r\n",
        "            #    loss='categorical_crossentropy',\r\n",
        "               loss='mse',\r\n",
        "#               loss='mae',\r\n",
        "               metrics=['acc'])\r\n",
        "\r\n",
        "#  CNN 학습 (verbose 결과를 출력할 것인가 말것인가)\r\n",
        "\r\n",
        "print(\"학습 시작\")\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x=X_train,\r\n",
        "          y=Y_train,\r\n",
        "          epochs=MY_EPOCH,\r\n",
        "          batch_size=MY_BATCH,\r\n",
        "          verbose=1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습시간: {:.2f}'.format(end-begin))\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 시작\n",
            "Epoch 1/100\n",
            "120/120 [==============================] - 4s 10ms/step - loss: 0.0483 - acc: 0.6547\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0206 - acc: 0.8591\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0180 - acc: 0.8779\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0166 - acc: 0.8871\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0150 - acc: 0.8990\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0142 - acc: 0.9057\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0138 - acc: 0.9064\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0133 - acc: 0.9116\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0123 - acc: 0.9176\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0123 - acc: 0.9181\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0117 - acc: 0.9223\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0112 - acc: 0.9262\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0110 - acc: 0.9276\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0108 - acc: 0.9286\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0101 - acc: 0.9346\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0100 - acc: 0.9341\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0093 - acc: 0.9396\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0093 - acc: 0.9406\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0090 - acc: 0.9414\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0083 - acc: 0.9466\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0086 - acc: 0.9457\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0082 - acc: 0.9470\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0078 - acc: 0.9523\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0076 - acc: 0.9523\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0074 - acc: 0.9540\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0066 - acc: 0.9602\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0067 - acc: 0.9582\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0062 - acc: 0.9627\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0062 - acc: 0.9624\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0059 - acc: 0.9640\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0054 - acc: 0.9675\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0055 - acc: 0.9661\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0054 - acc: 0.9675\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0052 - acc: 0.9686\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0050 - acc: 0.9696\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0047 - acc: 0.9730\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0045 - acc: 0.9735\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0042 - acc: 0.9763\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0046 - acc: 0.9724\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0039 - acc: 0.9784\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0041 - acc: 0.9768\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0045 - acc: 0.9734\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0035 - acc: 0.9807\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0034 - acc: 0.9812\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0031 - acc: 0.9830\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0033 - acc: 0.9814\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0032 - acc: 0.9827\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0029 - acc: 0.9848\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0030 - acc: 0.9845\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0028 - acc: 0.9844\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0028 - acc: 0.9848\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0031 - acc: 0.9823\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0026 - acc: 0.9861\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0027 - acc: 0.9854\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0024 - acc: 0.9876\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0022 - acc: 0.9888\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0022 - acc: 0.9885\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0021 - acc: 0.9891\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0022 - acc: 0.9885\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0022 - acc: 0.9879\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0025 - acc: 0.9864\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0021 - acc: 0.9885\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0024 - acc: 0.9865\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0024 - acc: 0.9865\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0022 - acc: 0.9878\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9897\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9899\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0017 - acc: 0.9906\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9906\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9899\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9907\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0020 - acc: 0.9891\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9902\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9899\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0020 - acc: 0.9896\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9903\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9903\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9902\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9895\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0016 - acc: 0.9919\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0017 - acc: 0.9905\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0020 - acc: 0.9889\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0016 - acc: 0.9914\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9894\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0017 - acc: 0.9913\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0014 - acc: 0.9924\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 0.9919\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 0.9918\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 0.9920\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9898\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0020 - acc: 0.9883\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0018 - acc: 0.9898\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0016 - acc: 0.9917\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0013 - acc: 0.9931\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0019 - acc: 0.9896\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0017 - acc: 0.9907\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0017 - acc: 0.9908\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 0.9920\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0016 - acc: 0.9915\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 1s 10ms/step - loss: 0.0015 - acc: 0.9921\n",
            "학습시간: 121.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypu8_mo1iCNW"
      },
      "source": [
        "# 위에서 120은 회수 (6만개를 500개씩 가져오면 batch는 120번 가져와야 함)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6cS9aymfLW_",
        "outputId": "c12f7d9d-34c9-4518-9ba6-a21c263f7f2a"
      },
      "source": [
        "# CNN  평가\r\n",
        "score = model.evaluate(x=X_test,\r\n",
        "                       y=Y_test,\r\n",
        "                       verbose=1)\r\n",
        "\r\n",
        "print('최종 손실값: {:.2f}'.format(score[0]))\r\n",
        "print('최종 정확도: {:.2f}'.format(score[1]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0144 - acc: 0.9136\n",
            "최종 손실값: 0.01\n",
            "최종 정확도: 0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZXG7LsUnhGa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO2x3PR5f_A-"
      },
      "source": [
        "6.  CNN  예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQyhO5nBgDhM",
        "outputId": "4886a9cd-5035-4f88-8e55-01180c47e159"
      },
      "source": [
        "# 샘플 이미지 지정\r\n",
        "image = X_test[1234]\r\n",
        "print(image.shape)\r\n",
        "\r\n",
        "# batch 정보 추가 (1: batch 1개씩 가져온다는 뜻)\r\n",
        "image = image.reshape(1, 28, 28, 1)\r\n",
        "pred = model.predict(image)\r\n",
        "\r\n",
        "# 정보 출력 (softmax 처리가 된 10개 확률)\r\n",
        "print('CNN 예측 값', pred)\r\n",
        "print('정답:', Y_test[1234])\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n",
            "CNN 예측 값 [[3.8960189e-12 5.5858369e-12 3.8311967e-01 4.4843141e-13 6.1679751e-01\n",
            "  3.4467600e-16 8.2784856e-05 2.5586510e-16 8.6447030e-12 1.8021623e-13]]\n",
            "정답: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "0mEg3RN5lPvJ",
        "outputId": "5d9016d6-2135-4b38-efad-a370d1e92602"
      },
      "source": [
        "# 1234   번째 데이터 그리기 (정규화 255 나눈거 다시 곱해줌)\r\n",
        "\r\n",
        "plt.figure(figsize=(10,10))\r\n",
        "tmp = X_train[1234].reshape(28,28)\r\n",
        "tmp = tmp*255\r\n",
        "plt.imshow(tmp, cmap='gray')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf65f1d400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}